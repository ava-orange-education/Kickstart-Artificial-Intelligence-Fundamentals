# Import necessary libraries
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.utils import np_utils, pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding

# Define the input data
data = """Mary had a little lamb, little lamb, little lamb. Mary had a little lamb
its fleece was white as snow. And everywhere that Mary went, Mary went, Mary went
everywhere that Mary went, the lamb was sure to go. It followed her to school one
day, school one day, school one day. It followed her to school one day, which was
against the rule. It made the children laugh and play, laugh and play, laugh and
play. It made the children laugh and play to see a lamb at school. So the teacher
turned him out, turned him out, turned him out. So the teacher turned him out and
sent him straight away"""

# Initialize the Tokenizer and fit it on the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([data])

# Encode the text into integers
encoded = tokenizer.texts_to_sequences([data])[0]
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary Size:', vocab_size)

# Create input-output sequences for training
sequences = []
for line in data.split('\n'):
    encoded = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(encoded)):
        sequence = encoded[:i+1]
        sequences.append(sequence)

print('Total Sequences:', len(sequences))

# Pad sequences to ensure equal length
max_length = max([len(seq) for seq in sequences])
sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')
print('Max Sequence Length:', max_length)

# Separate sequences into input (X) and output (y)
sequences = np.array(sequences)
X, y = sequences[:, :-1], sequences[:, -1]
y = np_utils.to_categorical(y, num_classes=vocab_size)

# Build the model
model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_length-1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))

# Print model summary
print(model.summary())

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=500, verbose=2)

# Define a function to generate text sequences
def generate_seq(model, tokenizer, max_length, seed_text, n_words):
    in_text = seed_text
    # Generate a fixed number of words
    for _ in range(n_words):
        # Encode the text as integers
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        # Pre-pad sequences to a fixed length
        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')
        # Predict probabilities for the next word
        yhat = np.argmax(model.predict(encoded), axis=-1)
        # Map predicted word index to the word
        out_word = ''
        for word, index in tokenizer.word_index.items():
            if index == yhat:
                out_word = word
                break
        # Append the predicted word to the input text
        in_text += ' ' + out_word
    return in_text

# Evaluate the model by generating a sequence
print(generate_seq(model, tokenizer, max_length-1, 'fleece was', 5))
