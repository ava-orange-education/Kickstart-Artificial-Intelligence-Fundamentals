# Import necessary libraries
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Model
from keras.layers import Input, LSTM, Dense, Embedding
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical

# Set hyperparameters
BATCH_SIZE = 64
EPOCHS = 30
LSTM_NODES = 256
NUM_SENTENCES = 20000
MAX_SENTENCE_LENGTH = 50
MAX_NUM_WORDS = 20000
EMBEDDING_SIZE = 100

# Load and preprocess the data
input_sentences = []
output_sentences = []
output_sentences_inputs = []
count = 0

# Open the French-English translation dataset
for line in open(r'/filepath/fra.txt', encoding="utf-8"):
    count += 1
    if count > NUM_SENTENCES:
        break
    if '\t' not in line:
        continue
    input_sentence, output, *rest = line.rstrip().split('\t')
    output_sentence = output + ' <eos>'  # Decoder output
    output_sentence_input = '<sos> ' + output  # Decoder input
    input_sentences.append(input_sentence)
    output_sentences.append(output_sentence)
    output_sentences_inputs.append(output_sentence_input)

print("Number of input samples:", len(input_sentences))
print("Number of output samples:", len(output_sentences))
print("Number of output input samples:", len(output_sentences_inputs))

# Print an example
print("Input Sentence Example:", input_sentences[200])
print("Output Sentence Example:", output_sentences[200])
print("Output Input Sentence Example:", output_sentences_inputs[200])

# Tokenize input sentences
input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
input_tokenizer.fit_on_texts(input_sentences)
input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)
word2idx_inputs = input_tokenizer.word_index
print('Total unique words in the input:', len(word2idx_inputs))

# Find the maximum length of input sentences
max_input_len = max(len(sen) for sen in input_integer_seq)
print("Max input sentence length:", max_input_len)

# Tokenize output sentences
output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')
output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)
output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)
output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)
word2idx_outputs = output_tokenizer.word_index
print('Total unique words in the output:', len(word2idx_outputs))

# Find the maximum length of output sentences
max_out_len = max(len(sen) for sen in output_integer_seq)
print("Max output sentence length:", max_out_len)

# Pad sequences for the encoder and decoder
encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)
decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')
decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')

print("Encoder input shape:", encoder_input_sequences.shape)
print("Decoder input shape:", decoder_input_sequences.shape)

# Word Embeddings using GloVe
embeddings_dictionary = dict()
glove_file = open(r'/filepath/glove.6B.100d.txt', encoding="utf8")
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions
glove_file.close()

# Create the embedding matrix
num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))
for word, index in word2idx_inputs.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Embedding layer for the encoder
embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)

# One-hot encode decoder outputs
decoder_targets_one_hot = np.zeros((len(input_sentences), max_out_len, len(word2idx_outputs) + 1), dtype='float32')
for i, d in enumerate(decoder_output_sequences):
    for t, word in enumerate(d):
        decoder_targets_one_hot[i, t, word] = 1

# Define the encoder
encoder_inputs_placeholder = Input(shape=(max_input_len,))
x = embedding_layer(encoder_inputs_placeholder)  # Embedding layer
encoder = LSTM(LSTM_NODES, return_state=True)
encoder_outputs, h, c = encoder(x)
encoder_states = [h, c]

# Define the decoder
decoder_inputs_placeholder = Input(shape=(max_out_len,))
decoder_embedding = Embedding(len(word2idx_outputs) + 1, EMBEDDING_SIZE)
decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)
decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)
decoder_dense = Dense(len(word2idx_outputs) + 1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Build and compile the model
model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
r = model.fit(
    [encoder_input_sequences, decoder_input_sequences],
    decoder_targets_one_hot,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    validation_split=0.1,
)

# Define the encoder model for inference
encoder_model = Model(encoder_inputs_placeholder, encoder_states)

# Define the decoder model for inference
decoder_state_input_h = Input(shape=(LSTM_NODES,))
decoder_state_input_c = Input(shape=(LSTM_NODES,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_inputs_single = Input(shape=(1,))
decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)
decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)
decoder_states = [h, c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model(
    [decoder_inputs_single] + decoder_states_inputs,
    [decoder_outputs] + decoder_states,
)

# Map indices to words
idx2word_input = {v: k for k, v in word2idx_inputs.items()}
idx2word_target = {v: k for k, v in word2idx_outputs.items()}

# Function to translate sentences
def translate_sentence(input_seq):
    states_value = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = word2idx_outputs['<sos>']
    eos = word2idx_outputs['<eos>']
    output_sentence = []
    for _ in range(max_out_len):
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        idx = np.argmax(output_tokens[0, 0, :])
        if eos == idx:
            break
        word = ''
        if idx > 0:
            word = idx2word_target[idx]
            output_sentence.append(word)
        target_seq[0, 0] = idx
        states_value = [h, c]
    return ' '.join(output_sentence)

# Test the model
i = 17820
input_seq = encoder_input_sequences[i:i+1]
translation = translate_sentence(input_seq)
print('-')
print('Input:', input_sentences[i])
print('Response:', translation)
